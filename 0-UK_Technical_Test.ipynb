{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links:\n",
    "https://www.kaggle.com/iamleonie/intro-to-time-series-forecasting\n",
    "https://www.kaggle.com/anshuls235/time-series-forecasting-eda-fe-modelling   \n",
    "https://machinelearningmastery.com/time-series-forecasting-methods-in-python-cheat-sheet/    \n",
    "https://towardsdatascience.com/ml-approaches-for-time-series-4d44722e48fe\n",
    "https://www.kaggle.com/dimitreoliveira/deep-learning-for-time-series-forecasting\n",
    "    https://machinelearningmastery.com/how-to-develop-lstm-models-for-multi-step-time-series-forecasting-of-household-power-consumption/\n",
    "https://www.kaggle.com/sejinsim/predictive-analysis-with-different-approaches  \n",
    "https://www.kaggle.com/sergeydor/web-traffic-time-series-forecast-with-4-model\n",
    "https://www.kaggle.com/fatmakursun/time-series-forecasting-unknown-future  \n",
    "https://www.kaggle.com/kyakovlev/m5-three-shades-of-dark-darker-magic\n",
    "https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/164599\n",
    "https://www.kaggle.com/chrisrichardmiles/simple-model-avg-last-28-days-grouped-by-weekday\n",
    "https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/164374\n",
    "https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163216\n",
    "https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163916\n",
    "https://www.kaggle.com/c/m5-forecasting-uncertainty/discussion/166875\n",
    "https://www.kaggle.com/c/m5-forecasting-uncertainty/discussion/166854\n",
    "https://www.kaggle.com/c/m5-forecasting-uncertainty/discussion/168053\n",
    "https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795 best !!!\n",
    "https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8125!!!\n",
    "https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/discussion/8023!!!\n",
    "https://www.kaggle.com/c/rossmann-store-sales/discussion/17896!!! \n",
    "https://www.kaggle.com/dimitreoliveira/deep-learning-for-time-series-forecasting    \n",
    "https://towardsdatascience.com/time-series-forecasting-predicting-stock-prices-using-facebooks-prophet-model-9ee1657132b5    \n",
    "https://www.kaggle.com/robikscube/tutorial-time-series-forecasting-with-xgboost\n",
    "https://www.kaggle.com/someadityamandal/bitcoin-time-series-forecasting\n",
    "https://www.kaggle.com/dimitreoliveira/time-series-forecasting-with-lstm-autoencoders \n",
    "https://www.kaggle.com/husammohammad/time-series-model-sarimax-vs-lstm-vs-fbprophet  \n",
    "        solar : \n",
    "            https://www.kaggle.com/c/ams-2014-solar-energy-prediction-contest/discussion/6321\n",
    "https://github.com/sjvasquez/web-traffic-forecasting\n",
    "https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43727    \n",
    "https://www.kaggle.com/someadityamandal/bitcoin-time-series-forecasting    \n",
    "https://pub.towardsai.net/finding-time-shift-between-two-timeseries-for-maximum-correlation-fc3010f1344e\n",
    "https://bmcemergmed.biomedcentral.com/articles/10.1186/s12873-020-00395-y\n",
    "https://www.sciencedirect.com/science/article/pii/S2352484\n",
    "https://www.xenonstack.com/blog/time-series-analysis/719308546\n",
    "https://www.hindawi.com/journals/jhe/2019/6123745/\n",
    "https://towardsai.net/p/deep-learning/beginners-guide-to-timeseries-forecasting-with-lstms-using-tensorflow-and-keras-364ea291909b\n",
    "https://towardsdatascience.com/keep-it-simple-keep-it-linear-a-linear-regression-model-for-time-series-5dbc83d89fc3  \n",
    "https://pub.towardsai.net/flight-delay-prediction-7cba01b0b0ab\n",
    "https://pub.towardsai.net/algorithmic-trading-models-machine-learning-part-1-284845cb4c84    \n",
    "https://pub.towardsai.net/enriching-sequential-lstm-model-with-non-sequential-features-7224b5262132\n",
    "https://www.datasciencecentral.com/profiles/blogs/linear-machine-learning-and-probabilistic-approaches-for-time\n",
    "https://www.youtube.com/watch?v=wqQKFu41FIw\n",
    "https://www.slideshare.net/GautierMarti/how-deep-generative-models-can-help-quants-reduce-the-risk-of-overfitting\n",
    "https://www.datarobot.com/blog/ai-in-financial-markets-part-1-beyond-the-market-predicting-magic-box/\n",
    "https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315\n",
    "https://medium.com/analytics-vidhya/no-lstms-cant-predict-stock-prices-11f10dcb35d6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approch \n",
    "One point of view I suggest you to consider is time series data is not IID (Independent and Identically distributed), hence it is always lags do play a significant role. In these scenarios, sticking to models like ARIMA (in case there are no explanatory variables present) is beneficial. In case if you feel there are significant explanatory variables through which you can feel like forecast well, then you can try with LSTM, Vector Auto regression or ARIMAX with exogenous variables ie beneficial. In case if you would like to test all the models apple to apple comparison, then I suggest you to split the test data separate and check MAPE/SMAPE accuracy of different models to get clear picture for yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Box-Jenkins method\n",
    "\n",
    "You've learned lots of tools and methods for working with and modeling time series. In this lesson you will learn about the best practices framework for using these tools.\n",
    "2. The Box-Jenkins method\n",
    "\n",
    "Building time series models can represent a lot of work for the modeler and so we want to maximize our ability to carry out these projects fast, efficiently and rigorously. This is where the Box-Jenkins method comes in. The Box-Jenkins method is a kind of checklist for you to go from raw data to a model ready for production. The three main steps that stand between you and a production-ready model are identification, estimation and model diagnostics.\n",
    "3. Identification\n",
    "\n",
    "In the identification step we explore and characterize the data to find some form of it which is appropriate to ARIMA modeling. We need to know whether the time series is stationary and find which transformations, such as differencing or taking the log of the data, will make it stationary. Once we have found a stationary form, we must identify which orders p and q are the most promising.\n",
    "4. Identification tools\n",
    "\n",
    "Our tools to test for stationarity include plotting the time series and using the augmented Dicky-Fuller test. Then we can take the difference or apply transformations until we find the simplest set of transformations that make the time series stationary. Finally we use the ACF and PACF to identify promising model orders.\n",
    "5. Estimation\n",
    "\n",
    "The next step is estimation, which involves using numerical methods to estimate the AR and MA coefficients of the data. Thankfully, this is automatically done for us when we call the model's dot-fit method. At this stage we might fit many models and use the AIC and BIC to narrow down to more promising candidates.\n",
    "6. Model diagnostics\n",
    "\n",
    "In the model diagnostics step, we evaluate the quality of the best fitting model. Here is where we use our test statistics and diagnostic plots to make sure the residuals are well behaved.\n",
    "7. Decision\n",
    "\n",
    "Using the information gathered from statistical tests and plots during the diagnostic step, we need to make a decision. Is the model good enough or do we need to go back and rework it.\n",
    "8. Repeat\n",
    "\n",
    "If the residuals aren't as they should be we will go back and rethink our choices in the earlier steps.\n",
    "9. Production\n",
    "\n",
    "If the residuals are okay then we can go ahead and make forecasts!\n",
    "10. Box-Jenkins\n",
    "\n",
    "This should be your general project workflow when developing time series models. You may have to repeat the process a few times in order to build a model that fits well. But as they say, no pain, no gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspecting your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "Bank = pd.read_csv('/home/abderrazak/ALLINHERE/NLP/Datacamp/world_ind_pop_data.csv')\n",
    "Bank.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
